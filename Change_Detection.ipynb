{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the generated event log in CSV format and classifies the event attributes according to their process characteristic. Further, it calculates the coefficient of variation(CV), which is used to measure the degree of variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from psycopg2 import connect\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from scipy.stats import variation\n",
    "from scipy import stats\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.statistics.eventually_follows.log import get as efg_get\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as sk\n",
    "from scipy.stats import chi2_contingency\n",
    "import math\n",
    "import statistics\n",
    "import pingouin as pg\n",
    "import graphviz\n",
    "from statsmodels.stats import multitest\n",
    "from statsmodels.stats.contingency_tables import SquareTable as ST\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load event log \n",
    "final_pm = pd.read_csv(\"Kidney_Failure_Log.csv\")\n",
    "hadms = list(final_pm[\"hadm_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'hadm_id'}\n",
    "event_log = pm4py.format_dataframe(final_pm, case_id='hadm_id', activity_key='department', timestamp_key='intime')\n",
    "log = pm4py.convert_to_event_log(event_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve all possible process variants and remove variants occuring < 20 times due to their small sample size\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "variants = variants_filter.get_variants(log)\n",
    "variants = list(variants.keys())\n",
    "var = final_pm.groupby('hadm_id')['department'].apply(list).reset_index()\n",
    "var[\"department\"] = var['department'].apply(lambda x: ','.join(map(str, x)))\n",
    "var = var.rename({\"department\":\"variant\"}, axis=1)\n",
    "final_pm_var = final_pm.merge(var, how=\"left\", on=\"hadm_id\")\n",
    "var_count= final_pm_var.drop_duplicates(\"hadm_id\").groupby(\"variant\").count()\n",
    "to_drop = list(var_count.loc[var_count[\"hadm_id\"] < 20].reset_index()[\"variant\"])\n",
    "for ele in to_drop:\n",
    "    variants.remove(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_attributes(proc_c):\n",
    "    for index, row in proc_c.iterrows():\n",
    "        if((row[\"numberOfActivities\"] == 1) & (row[\"numberOfTraceOccurence (Mean)\"] == 1)):\n",
    "            proc_c.at[index, \"class\"] = \"static\"\n",
    "        elif((row[\"numberOfActivities\"] > 1) & (row[\"numberOfTraceOccurence (Mean)\"] == 1)):\n",
    "            proc_c.at[index, \"class\"] = \"semi-dynamic\"\n",
    "        else:\n",
    "            proc_c.at[index, \"class\"] = \"dynamic\"\n",
    "    return proc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify activity column\n",
    "activity = \"department\"\n",
    "#specify case id\n",
    "case_id = \"hadm_id\"\n",
    "#specify attributes which should not be classified\n",
    "columns_to_drop = ['Unnamed: 0','subject_id','transfer_id','intime','outtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classify event attributes, so that dynamic event attributes can be identified\n",
    "final_pm = final_pm.drop(columns_to_drop, axis=1)\n",
    "\n",
    "activities = final_pm[activity].unique()\n",
    "\n",
    "matrix = pd.DataFrame(data=None, columns=activities)\n",
    "\n",
    "#identify attributes for activities\n",
    "att_card = pd.DataFrame(data=None,columns=final_pm.columns)\n",
    "for dep in activities:\n",
    "    dep_data = final_pm.loc[final_pm[activity] == dep]\n",
    "    y = dep_data.groupby(activity).agg({lambda x: x.notnull().sum()})\n",
    "    y.columns = y.columns.droplevel(1)\n",
    "    y = y.reset_index().drop(activity, axis=1)\n",
    "    row_num = len(dep_data)\n",
    "    row = y.loc[0]\n",
    "    for col in y.columns:\n",
    "        t = 0.05\n",
    "        if(row[col] > (row_num*t)):\n",
    "            row[col] = 1\n",
    "        else:\n",
    "            row[col] = 0\n",
    "    row[activity] = dep\n",
    "    att_card = att_card.append(row)\n",
    "    \n",
    "\n",
    "\n",
    "att_card.drop(case_id, axis=1, inplace=True)\n",
    "\n",
    "# for each attribute: number of activities + number of occurence in a trace\n",
    "\n",
    "number_trace_occurence = final_pm.groupby(case_id).agg({lambda x: x.notnull().sum()})\n",
    "\n",
    "#drop concept:name\n",
    "number_trace_occurence.drop(activity, axis=1, inplace=True)\n",
    "\n",
    "number_trace_occurence.columns = number_trace_occurence.columns.droplevel(1)\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.replace(0, np.NaN)\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.mean()\n",
    "\n",
    "number_trace_occurence = number_trace_occurence.rename(\"numberOfTraceOccurence (Mean)\")\n",
    "\n",
    "number_of_activities = pd.Series([], name=\"numberOfActivities\")\n",
    "\n",
    "for col in final_pm.columns:\n",
    "    if((col != case_id) & (col != activity)):\n",
    "        number_of_activities[col] = len(final_pm[[activity, col]].dropna()[activity].unique())\n",
    "\n",
    "process_characteristics = pd.concat([number_of_activities, number_trace_occurence], axis=1)\n",
    "\n",
    "for col in final_pm.columns:\n",
    "    if (final_pm[col].nunique()/final_pm[col].count() < 0.05):\n",
    "        process_characteristics.loc[col, \"type\"] = \"categorical\"\n",
    "    else:\n",
    "        process_characteristics.loc[col, \"type\"] = \"continuous\"\n",
    "\n",
    "process_characteristics = process_characteristics.drop(labels=[case_id, activity])\n",
    "\n",
    "x = process_characteristics\n",
    "\n",
    "x = classify_attributes(process_characteristics)\n",
    "\n",
    "x = x.reset_index()\n",
    "\n",
    "x = x.rename({\"index\":\"Activity\"}, axis=1)\n",
    "\n",
    "attribute_classes = x[[\"Activity\", \"class\", \"type\"]]\n",
    "\n",
    "attribute_classes[\"CV\"] = 0\n",
    "\n",
    "deps = [\"Emergency Department\", \"Pre-ICU Medicine\", \"Pre-ICU Cardiology\", \"Cardiac ICU\", \"Medical ICU\", \"Surgical ICU\", \"Post-ICU Cardiology\", \"Post-ICU Medicine\", \"Post-ICU Surgery\", \"Discharged\"]\n",
    "\n",
    "attribute_list_con = list(attribute_classes.loc[(attribute_classes[\"class\"] == \"dynamic\") & (attribute_classes[\"type\"] == \"continuous\")][\"Activity\"])\n",
    "\n",
    "attribute_list_cat = list(attribute_classes.loc[(attribute_classes[\"class\"] == \"dynamic\") & (attribute_classes[\"type\"] == \"categorical\")][\"Activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = dfg_discovery.apply(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove small sample size relations (optional)\n",
    "l = list()\n",
    "for x in dfg:\n",
    "    if((\"Pre-ICU Surgery\" in x[0]) | (\"Pre-ICU Surgery\" in x[1])):\n",
    "        l.append(x)\n",
    "    elif (dfg[x] <= 30):\n",
    "        l.append(x)\n",
    "for e in l:\n",
    "    del(dfg[e])       \n",
    "\n",
    "efg_graph = efg_get.apply(log)\n",
    "\n",
    "#remove small sample size relations (optional)\n",
    "l = list()\n",
    "for x in efg_graph:\n",
    "    if((\"Pre-ICU Surgery\" in x[0]) | (\"Pre-ICU Surgery\" in x[1])):\n",
    "        l.append(x)\n",
    "    elif (efg_graph[x] <= 30):\n",
    "        l.append(x)\n",
    "for e in l:\n",
    "    del(efg_graph[e])       \n",
    "\n",
    "l = list()\n",
    "for ele in efg_graph:\n",
    "    if(ele in dfg):\n",
    "        l.append(ele)\n",
    "for e in l:\n",
    "    del(efg_graph[e])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have directly and eventually follow relations + info about where which attribute is used (att_card) \n",
    "#+ process characteristics (attribute_classes)\n",
    "#perform statistical tests now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_hadms(df, act_1, act_2):\n",
    "    df = df.loc[df[\"department\"].isin([act_1, act_2])]\n",
    "    curr_hadm = 0\n",
    "    index_1 = 0\n",
    "    l = []\n",
    "    for index, row in df.iterrows():\n",
    "        #first row\n",
    "        if(curr_hadm != row[\"hadm_id\"]):\n",
    "            curr_hadm = row[\"hadm_id\"]\n",
    "            index_1 = index\n",
    "        else:\n",
    "            if(index - index_1 == 1):\n",
    "                l.append(row[\"hadm_id\"])\n",
    "    return df.loc[df[\"hadm_id\"].isin(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eventually_follow_hadms(df, act_1, act_2):\n",
    "    df = df.loc[df[\"department\"].isin([act_1, act_2])]\n",
    "    curr_hadm = 0\n",
    "    l = []\n",
    "    for index, row in df.iterrows():\n",
    "        #first row\n",
    "        if(curr_hadm != row[\"hadm_id\"]):\n",
    "            curr_hadm = row[\"hadm_id\"]\n",
    "        else:\n",
    "            l.append(row[\"hadm_id\"])\n",
    "    return df.loc[df[\"hadm_id\"].isin(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_value_con(dep_1, dep_2, ea, df):\n",
    "    df_wo_na = df.loc[~df[ea].isna()]\n",
    "    summary = df_wo_na.groupby(\"hadm_id\").count()\n",
    "    df_wo_na = summary.loc[(summary[\"department\"] > 1) & (summary[\"department\"] < 3)]\n",
    "    hadms_wo_na = list(df_wo_na.reset_index()[\"hadm_id\"])\n",
    "    df_wo_na = df.loc[df[\"hadm_id\"].isin(hadms_wo_na)]\n",
    "    df = df_wo_na\n",
    "    l1 = list(df.loc[(df[\"department\"] == dep_1) & (~df[ea].isna())][ea])\n",
    "    l2 = list(df.loc[(df[\"department\"] == dep_2) & (~df[ea].isna())][ea])\n",
    "    df1 = df.loc[(df[\"department\"] == dep_1) & (~df[ea].isna())]\n",
    "    df2 = df.loc[(df[\"department\"] == dep_2) & (~df[ea].isna())]\n",
    "    \n",
    "    if((len(l1) < 8) | (len(l2) < 8)):\n",
    "        return(np.nan,np.nan, np.nan, np.nan,np.nan,np.nan, np.nan, np.nan)\n",
    "    try:\n",
    "        p = pg.wilcoxon(l1, l2)[\"p-val\"][0]\n",
    "        cles = pg.wilcoxon(l1, l2)[\"CLES\"][0]\n",
    "        rbc = pg.wilcoxon(l1, l2)[\"RBC\"][0]\n",
    "        z = stats.norm.isf(p / 2)\n",
    "        r = z / np.sqrt(len(l1)*2)        \n",
    "        cohen = 2*r / np.sqrt(1-np.square(r))\n",
    "        return (p, cles, rbc, len(l1), df1[ea].mean(), df2[ea].mean(), df1[ea].std(), df2[ea].std())\n",
    "    except:\n",
    "        return(1,0,0,0, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_All = pd.DataFrame()\n",
    "df_con = pd.DataFrame()\n",
    "for rel in dfg:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = consecutive_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[\"department\"].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_con))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, consecutive_df)\n",
    "        con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)    \n",
    "        if(p <= (0.05 / len(att_list))):\n",
    "            df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, df_var)\n",
    "            con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "            if(p <= (0.05 / len(att_list))):\n",
    "                df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':True}, ignore_index=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in efg_graph:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = eventually_follow_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[\"department\"].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_con))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, consecutive_df)\n",
    "        con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)    \n",
    "        if(p <= (0.05 / len(att_list))):\n",
    "            df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : 'ALL', '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "            for var in variants:\n",
    "                df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "                p, cles, rbc, num_p, m1, m2, st1, st2 = stat_value_con(rel[0], rel[1], e_at, df_var)\n",
    "                con_All = con_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "                if(p <= (0.05 / len(att_list))):\n",
    "                    df_con = df_con.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"RBC\": rbc, 'abs(RBC)': abs(rbc), 'var' : var, '#Patients' : num_p, 'M1':m1, 'M2':m2, 'ST1':st1, 'ST2':st2, 'Directly':False}, ignore_index=True)\n",
    "\n",
    "con_All = con_All.loc[~con_All[\"P\"].isna()]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuart_maxwell(cons_df, dep1, dep2, att):\n",
    "    graph_stats = cons_df[[\"hadm_id\", \"department\", att]]\n",
    "    to_remove = graph_stats.loc[graph_stats[att].isna()][\"hadm_id\"]\n",
    "    graph_stats = graph_stats.loc[~graph_stats[\"hadm_id\"].isin(to_remove)]\n",
    "    curr_hadm = \"\"\n",
    "    first_val = \"\"\n",
    "    second_val = \"\"\n",
    "    abnormal_col = graph_stats.columns[2]\n",
    "    val_count = graph_stats[abnormal_col].value_counts()\n",
    "    graph_cat = pd.DataFrame(columns=[\"Source\", \"Target\", \"Frequency\"])\n",
    "    for col_source in val_count.index:\n",
    "        for col_target in val_count.index:\n",
    "            new_row = {\"Source\":col_source, \"Target\":col_target, \"Frequency\": 0}\n",
    "            graph_cat = graph_cat.append(new_row, ignore_index=True)\n",
    "    for index, row in graph_stats.iterrows():\n",
    "        if(curr_hadm != row[\"hadm_id\"]):\n",
    "            curr_hadm = row[\"hadm_id\"]\n",
    "            first_val = row[abnormal_col]\n",
    "        else:\n",
    "            second_val = row[abnormal_col]\n",
    "            if((pd.isna(first_val)) | (pd.isna(second_val))):\n",
    "                pass\n",
    "            else:\n",
    "                freq = graph_cat.loc[(graph_cat[\"Source\"] == first_val) & (graph_cat[\"Target\"] == second_val)][\"Frequency\"].iloc[0]\n",
    "                graph_cat.loc[(graph_cat[\"Source\"] == first_val) & (graph_cat[\"Target\"] == second_val), \"Frequency\"] = freq+1\n",
    "    tab = graph_cat.set_index(['Source', 'Target'])\n",
    "    tab = tab.unstack()\n",
    "    tab.columns = tab.columns.get_level_values(1)\n",
    "    sqtab = ST(tab)\n",
    "    test = sqtab.homogeneity()\n",
    "    p = test.pvalue\n",
    "    chi2 = test.statistic\n",
    "    return tab, p, chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_value_cat(dep_1, dep_2, ea, df):\n",
    "    df_wo_na = df.loc[~df[ea].isna()]\n",
    "    summary = df_wo_na.groupby(\"hadm_id\").count()\n",
    "    df_wo_na = summary.loc[(summary[\"department\"] > 1) & (summary[\"department\"] < 3)]\n",
    "    hadms_wo_na = list(df_wo_na.reset_index()[\"hadm_id\"])\n",
    "    df_wo_na = df.loc[df[\"hadm_id\"].isin(hadms_wo_na)]\n",
    "    df = df_wo_na\n",
    "    num_p = len(df.loc[(df[\"department\"] == dep_1) & (~df[ea].isna())])\n",
    "    count_1 = df.loc[(df[\"department\"] == dep_1) & (~df[ea].isna())][ea].value_counts()\n",
    "    count_2 = df.loc[(df[\"department\"] == dep_2) & (~df[ea].isna())][ea].value_counts()\n",
    "    if((len(count_1) < 2) | (len(count_2) < 2)):\n",
    "        return(np.nan,np.nan, np.nan)\n",
    "    g, p, chi2 = stuart_maxwell(df, dep_1, dep_2, ea)\n",
    "    return (p, chi2, num_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_All = pd.DataFrame()\n",
    "df_cat = pd.DataFrame()\n",
    "for rel in dfg:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = consecutive_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[\"department\"].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_cat))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "        if(p <= (0.05) / len(att_list)):\n",
    "            df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, df_var)\n",
    "            cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "            if(p <= (0.05) / len(att_list)):\n",
    "                df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':True}, ignore_index=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in efg_graph:\n",
    "    #varianten aus consecutive df extrahieren\n",
    "    consecutive_df = eventually_follow_hadms(final_pm_var, rel[0], rel[1])\n",
    "    variants = consecutive_df[\"variant\"].unique()\n",
    "    att_list = att_card.loc[att_card[\"department\"].isin([rel[0], rel[1]])].sum().to_frame().reset_index()\n",
    "    att_list = att_list.rename({\"index\":\"e_At\", 0:\"cardinality\"}, axis=1)\n",
    "    att_list = att_list.loc[(att_list[\"cardinality\"] == 2) & (att_list[\"e_At\"].isin(attribute_list_cat))].reset_index()\n",
    "    for e_at in att_list[\"e_At\"]:\n",
    "        stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, consecutive_df)\n",
    "        cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "        if(p <= (0.05) / len(att_list)):\n",
    "            df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : 'ALL', '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "        for var in variants:\n",
    "            df_var = consecutive_df.loc[consecutive_df[\"variant\"] == var]\n",
    "            p, chi2, num_p = stat_value_cat(rel[0], rel[1], e_at, df_var)\n",
    "            cat_All = cat_All.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "            if(p <= (0.05)/ len(att_list)):\n",
    "                df_cat = df_cat.append({'Act_1': rel[0], 'Act_2': rel[1], 'E_At': e_at, 'P': p, \"Chi2\": chi2, 'var' : var, '#Patients' : num_p, 'Directly':False}, ignore_index=True)\n",
    "cat_All = cat_All.loc[~cat_All[\"P\"].isna()]         \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_All.to_csv(\"con_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_All.to_csv(\"cat_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat[\"Con_E_At\"] = df_cat[\"E_At\"].str.split(' ', 1, expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cat.to_csv(\"df_cat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_con.to_csv(\"df_con.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pm_var.to_csv(\"Kidney_Failure_Variant.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
